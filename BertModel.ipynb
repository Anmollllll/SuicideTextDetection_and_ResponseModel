{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPASDzxpZN1HnF4TEuCaFdz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anmollllll/SuicideTextDetection_and_ResponseModel/blob/main/BertModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install clean-text\n",
        "!pip install unidecode"
      ],
      "metadata": {
        "id": "XCbv3o1dr6kX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1N208pGVRH1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Changing to own directory\n",
        "os.chdir(\"/content/drive/MyDrive/SusDetect\")\n",
        "print(\"Directory changed\")\n",
        "\n",
        "# Loading dataset\n",
        "df = pd.read_csv('SusReddit2023dataset.csv')\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "id0yxCLPVmgb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Cleaning"
      ],
      "metadata": {
        "id": "ueGpN_0UbL0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=['Title'],inplace=True)"
      ],
      "metadata": {
        "id": "P1F1UvlcbF4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "LlbkdjA-bVmG",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking null values\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "UON3E2GsbXcn",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping null values\n",
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "yc_wAbhkbbST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking duplicate posts\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "fIqcmkgXciHO",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the duplicate rows\n",
        "duplicates = df[df.duplicated()]\n",
        "print(duplicates)"
      ],
      "metadata": {
        "id": "Sp9tqekwc8pa",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates(keep='first',inplace=True)"
      ],
      "metadata": {
        "id": "aXI9GdTDfBLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)"
      ],
      "metadata": {
        "id": "O3vZWHoafr7W",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "3NmljT10gK5Z",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing & EDA\n"
      ],
      "metadata": {
        "id": "IPuwHypJEYjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Label'].value_counts()"
      ],
      "metadata": {
        "id": "Zx5YDJBDgxkv",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = df[df['Label'] != 'Label']"
      ],
      "metadata": {
        "id": "jqR0scY6g2Bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Label'].value_counts()"
      ],
      "metadata": {
        "id": "8bIXJ078jGbW",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We can see the binary class is well balanced\n",
        "#Now using label encoder to encode the labels\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "df['Label']=encoder.fit_transform(df['Label'])"
      ],
      "metadata": {
        "id": "ad1y1nxqjI9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Label'].value_counts()"
      ],
      "metadata": {
        "id": "s-pzTlCJnIRU",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "rFFbdZZoWHhJ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "class TextQualityAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.encoding_artifacts = [\n",
        "            'â€™', 'â€œ', 'â€', 'â€\"', 'â€\"', 'Ã', 'Â', 'ï¿½',\n",
        "            'Ã¡', 'Ã©', 'Ã­', 'Ã³', 'Ãº', 'Ã±', 'â€¦'\n",
        "        ]\n",
        "\n",
        "        self.suspicious_patterns = {\n",
        "            'html_tags': r'<[^>]+>',\n",
        "            'excessive_punctuation': r'[!?]{4,}|[.]{4,}',\n",
        "            'excessive_caps': r'\\b[A-Z]{10,}\\b',\n",
        "            'urls': r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\n",
        "            'emails': r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',\n",
        "            'phone_numbers': r'(\\+\\d{1,3}[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',\n",
        "            'excessive_whitespace': r'\\s{3,}',\n",
        "            'repeated_words': r'\\b(\\w+)(\\s+\\1){3,}\\b',\n",
        "            'special_chars': r'[^\\w\\s\\.,!?;:\\'\"()\\-/_]',\n",
        "            'control_chars': r'[\\x00-\\x1f\\x7f-\\x9f]'\n",
        "        }\n",
        "\n",
        "    def analyze_single_text(self, text):\n",
        "        \"\"\"Analyze a single text for quality issues\"\"\"\n",
        "        if pd.isna(text):\n",
        "            return {\n",
        "                'is_empty': True,\n",
        "                'length': 0,\n",
        "                'word_count': 0,\n",
        "                'issues': ['empty_text']\n",
        "            }\n",
        "\n",
        "        text_str = str(text)\n",
        "        issues = []\n",
        "\n",
        "\n",
        "        # Basic statistics\n",
        "        length = len(text_str)\n",
        "        words = text_str.split()\n",
        "        word_count = len(words)\n",
        "\n",
        "        # Check for emptiness\n",
        "        if length == 0:\n",
        "            issues.append('empty_text')\n",
        "\n",
        "        # Check for encoding issues\n",
        "        encoding_issues = [artifact for artifact in self.encoding_artifacts if artifact in text_str]\n",
        "        if encoding_issues:\n",
        "            issues.append('encoding_artifacts')\n",
        "\n",
        "        # Check for suspicious patterns\n",
        "        for issue_name, pattern in self.suspicious_patterns.items():\n",
        "            if re.search(pattern, text_str):\n",
        "                issues.append(issue_name)\n",
        "\n",
        "        # Length-based issues\n",
        "        if word_count < 3:\n",
        "            issues.append('too_short')\n",
        "        elif word_count > 512:\n",
        "            issues.append('too_long')\n",
        "\n",
        "        # Repetition analysis\n",
        "        if word_count > 0:\n",
        "            word_freq = Counter([word.lower() for word in words])\n",
        "            most_common_freq = word_freq.most_common(1)[0][1] if word_freq else 0\n",
        "            if most_common_freq > word_count * 0.3:\n",
        "                issues.append('highly_repetitive')\n",
        "\n",
        "        # Character diversity\n",
        "        # Detect texts with very low character diversity (like \"aaaaaaa\" or repeated patterns)\n",
        "        unique_chars = len(set(text_str))\n",
        "        if unique_chars < 10 and length > 50:\n",
        "            issues.append('low_char_diversity')\n",
        "\n",
        "        # Language detection (basic)\n",
        "        # Detect text that's primarily in non-English languages or contains lots of special Unicode characters\n",
        "        ascii_ratio = sum(1 for c in text_str if ord(c) < 128) / length if length > 0 else 0\n",
        "        if ascii_ratio < 0.8:\n",
        "            issues.append('non_ascii_heavy')\n",
        "\n",
        "        return {\n",
        "            'is_empty': length == 0,\n",
        "            'length': length,\n",
        "            'word_count': word_count,\n",
        "            'unique_chars': unique_chars,\n",
        "            'ascii_ratio': ascii_ratio,\n",
        "            'encoding_artifacts': encoding_issues,\n",
        "            'issues': issues\n",
        "        }\n",
        "\n",
        "    def analyze_dataframe(self, df, text_column):\n",
        "        \"\"\"Analyze entire dataframe for text quality issues\"\"\"\n",
        "        results = []\n",
        "\n",
        "        print(f\"Analyzing {len(df)} texts...\")\n",
        "\n",
        "        for idx, text in enumerate(df[text_column]):\n",
        "            if idx % 1000 == 0:\n",
        "                print(f\"Processed {idx}/{len(df)} texts\")\n",
        "\n",
        "            analysis = self.analyze_single_text(text)\n",
        "            analysis['index'] = idx\n",
        "            analysis['text_preview'] = str(text) if not pd.isna(text) else 'NaN'\n",
        "            results.append(analysis)\n",
        "\n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "    def generate_report(self, analysis_df, sample_size=5):\n",
        "        \"\"\"Generate comprehensive quality report\"\"\"\n",
        "        print(\"=\" * 60)\n",
        "        print(\"TEXT QUALITY ANALYSIS REPORT\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        total_texts = len(analysis_df)\n",
        "        # Overall statistics\n",
        "        print(f\"\\n OVERALL STATISTICS:\")\n",
        "        print(f\"Total texts analyzed: {total_texts}\")\n",
        "        print(f\"Average length: {analysis_df['length'].mean():.1f} characters\")\n",
        "        print(f\"Average word count: {analysis_df['word_count'].mean():.1f} words\")\n",
        "        print(f\"Texts with issues: {len(analysis_df[analysis_df['issues'].str.len() > 0])}\")\n",
        "\n",
        "        # Issue frequency\n",
        "        all_issues = []\n",
        "        for issues_list in analysis_df['issues']:\n",
        "            all_issues.extend(issues_list)\n",
        "\n",
        "        issue_counts = Counter(all_issues)\n",
        "\n",
        "        print(f\"\\n TOP ISSUES FOUND:\")\n",
        "        for issue, count in issue_counts.most_common(10):\n",
        "            percentage = (count / total_texts) * 100\n",
        "            print(f\"  {issue}: {count} texts ({percentage:.1f}%)\")\n",
        "\n",
        "        # Examples for each major issue\n",
        "        print(f\"\\n SAMPLE PROBLEMATIC TEXTS:\")\n",
        "        for issue, count in issue_counts.most_common(5):\n",
        "            print(f\"\\n--- {issue.upper()} (Found in {count} texts) ---\")\n",
        "\n",
        "            # Find examples of this issue\n",
        "            examples = analysis_df[analysis_df['issues'].apply(lambda x: issue in x)]\n",
        "\n",
        "            for i, (_, row) in enumerate(examples.head(sample_size).iterrows()):\n",
        "                print(f\"  Example {i+1} (Index {row['index']}):\")\n",
        "                print(f\"    Text: {row['text_preview']}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "yvSai97eMf7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "analyzer = TextQualityAnalyzer()\n",
        "analysis_results = analyzer.analyze_dataframe(df, 'Post')\n",
        "issue_summary = analyzer.generate_report(analysis_results)"
      ],
      "metadata": {
        "id": "87ERp1XaVwcx",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cleantext\n",
        "\n",
        "def clean_dataset(text):\n",
        "\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = str(text)\n",
        "\n",
        "    text = cleantext.clean(text,\n",
        "        fix_unicode=True,\n",
        "        lower=True,\n",
        "        no_line_breaks=True,\n",
        "        no_urls=True,\n",
        "        no_emails=True,\n",
        "        no_phone_numbers=True,\n",
        "        no_numbers=False,\n",
        "        no_punct=False,\n",
        "        normalize_whitespace=True,\n",
        "    )\n",
        "\n",
        "    # Additional patterns cleantext doesn't handle\n",
        "    text = re.sub(r'<[^>]+>', ' ', text)   # HTML tags\n",
        "    text = re.sub('[^\\w\\s\\.,!?;:\\'\"()\\-/_]',' ',text)  # special characters\n",
        "    text = re.sub(r'([!?\\.]){4,}', r'\\1\\1\\1', text)   # Excessive punctuation (max 3)\n",
        "    text = re.sub(r'([.]){4,}', r'\\1\\1\\1', text)   # Excessive periods\n",
        "    text = re.sub(r'\\b[A-Z]{10,}\\b', lambda m: m.group().lower(), text)  # Excessive caps\n",
        "    text = re.sub(r'\\b(\\w+)(\\s+\\1){3,}\\b', r'\\1', text)    # Repeated words\n",
        "\n",
        "\n",
        "    # Final whitespace cleanup (in case regex added extra spaces)\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "6ezGpS1KYFLS",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['cleaned_text'] = df['Post'].apply(clean_dataset)"
      ],
      "metadata": {
        "id": "NH-ul7ygZzpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "DbeVHGowbh0E",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer1 = TextQualityAnalyzer()\n",
        "analysis_results1 = analyzer1.analyze_dataframe(df, 'cleaned_text')\n",
        "issue_summary1 = analyzer1.generate_report(analysis_results1)\n"
      ],
      "metadata": {
        "id": "wOazUeGKq9oH",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = df[['cleaned_text','Label']]"
      ],
      "metadata": {
        "id": "lBK8JWJbkMkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.head()"
      ],
      "metadata": {
        "id": "wLp231Z7X8bG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Visualization"
      ],
      "metadata": {
        "id": "RmoqMc0yXdtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "def bert_eda(df, text_col, label_col):\n",
        "\n",
        "    # Text Length Analysis\n",
        "    df['text_length'] = df[text_col].apply(len)\n",
        "    df['word_count'] = df[text_col].apply(lambda x: len(x.split()))\n",
        "\n",
        "    print(\"\\n--- Text Length and Word Count Analysis ---\")\n",
        "\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.histplot(df['text_length'], bins=50, kde=True)\n",
        "    plt.title('Distribution of Text Lengths')\n",
        "    plt.xlabel('Character Count')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.histplot(df['word_count'], bins=50, kde=True)\n",
        "    plt.title('Distribution of Word Counts')\n",
        "    plt.xlabel('Word Count')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(\"\\nText length and word count by label:\")\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    sns.histplot(data=df, x='text_length', hue=label_col, kde=True, ax=axes[0])\n",
        "    axes[0].set_title(f'Text Length Distribution by {label_col}')\n",
        "    axes[0].set_xlabel('Character Count')\n",
        "    axes[0].set_ylabel('Frequency')\n",
        "\n",
        "    sns.histplot(data=df, x='word_count', hue=label_col, kde=True, ax=axes[1])\n",
        "    axes[1].set_title(f'Word Count Distribution by {label_col}')\n",
        "    axes[1].set_xlabel('Word Count')\n",
        "    axes[1].set_ylabel('Frequency')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    #Vocabulary Analysis\n",
        "    print(\"\\n---  Vocabulary Analysis ---\")\n",
        "    all_words = ' '.join(df[text_col]).lower()\n",
        "    # Remove punctuation\n",
        "    all_words = re.sub(r'[^\\w\\s]', '', all_words)\n",
        "    words = all_words.split()\n",
        "    word_counts = Counter(words)\n",
        "    print(f\"Total unique words in corpus: {len(word_counts)}\")\n",
        "\n",
        "    #Top words by label\n",
        "    print(\"\\n--- Top 30 Most Common Words by Label ---\")\n",
        "    suicidal = []\n",
        "    non_suicidal = []\n",
        "    for text in new_df[new_df['Label']==1]['cleaned_text'].to_list():\n",
        "      for wordd in text.split():\n",
        "        suicidal.append(wordd)\n",
        "    for text in new_df[new_df['Label']==0]['cleaned_text'].to_list():\n",
        "      for wordd in text.split():\n",
        "        non_suicidal.append(wordd)\n",
        "\n",
        "    print(\"\\n--- For label-Suicidal ---\")\n",
        "    sns.barplot(x=pd.DataFrame(Counter(suicidal).most_common(30))[0],y=pd.DataFrame(Counter(suicidal).most_common(30))[1])\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n--- For label-Non_Suicidal ---\")\n",
        "    sns.barplot(x=pd.DataFrame(Counter(non_suicidal).most_common(30))[0],y=pd.DataFrame(Counter(non_suicidal).most_common(30))[1])\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # Clean up added columns\n",
        "    df.drop(columns=['text_length', 'word_count', 'has_url', 'has_hashtag', 'has_mention'], inplace=True, errors='ignore')\n"
      ],
      "metadata": {
        "id": "_QaKNMV3k2gH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_eda(new_df,'cleaned_text','Label')"
      ],
      "metadata": {
        "id": "494RUm3xqQQ_",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Building"
      ],
      "metadata": {
        "id": "EuTvBebJz7_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer,BertForSequenceClassification,Trainer,TrainingArguments\n",
        "import torch\n",
        "from torch.utils.data import Dataset"
      ],
      "metadata": {
        "id": "oALjhz1EnoVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(new_df['cleaned_text'],new_df['Label'],test_size=0.2,random_state=42,shuffle=True,stratify=df['Label'])"
      ],
      "metadata": {
        "id": "m3D2XkkD0Qsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "W_I0bHfy1IBA",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = tokenizer(X_train.tolist(),truncation=True,padding=True,max_length=512)\n",
        "test_encodings = tokenizer(X_test.tolist(),truncation=True,padding=True,max_length=512)"
      ],
      "metadata": {
        "id": "kSkUQZoJ1T5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "  def __init__(self,encodings,labels):\n",
        "    self.encodings = encodings\n",
        "    self.labels = labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    item = {key:torch.tensor(val[idx]) for key,val in self.encodings.items()}\n",
        "    item['labels'] = torch.tensor(self.labels[idx])\n",
        "    return item\n"
      ],
      "metadata": {
        "id": "khOu2sDh1zjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TextDataset(train_encodings, y_train.reset_index(drop=True))\n",
        "val_dataset = TextDataset(test_encodings, y_test.reset_index(drop=True))"
      ],
      "metadata": {
        "id": "d5zAD1rh2k6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6THzoMVx21hk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,\n",
        ")"
      ],
      "metadata": {
        "id": "r6qlWmQ13Ub9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "UpKq9hPs3ZLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "QsiINauo3bZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = trainer.evaluate()\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "euNuKuojJG34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EVALUATING EACH EPOCH CHECKPOINT\")\n",
        "print(\"=\"*70)\n",
        "def evaluate_model(trainer, eval_dataset, dataset_name):\n",
        "\n",
        "    # Get predictions\n",
        "    predictions = trainer.predict(eval_dataset)\n",
        "    preds = np.argmax(predictions.predictions, axis=1)\n",
        "    labels = predictions.label_ids\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"{dataset_name} Set Metrics:\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1-Score:  {f1:.4f}\")\n",
        "    print(f\"\\nDetailed Classification Report:\")\n",
        "    print(classification_report(labels, preds, target_names=['Class 0', 'Class 1']))\n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(labels, preds))\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }\n",
        "\n",
        "# all checkpoint directories\n",
        "checkpoint_dirs = sorted([d for d in os.listdir(\"./results\") if d.startswith(\"checkpoint-\")])\n",
        "\n",
        "epoch_results = []\n",
        "\n",
        "for i, checkpoint_dir in enumerate(checkpoint_dirs, 1):\n",
        "    checkpoint_path = os.path.join(\"./results\", checkpoint_dir)\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Epoch {i} - Checkpoint: {checkpoint_dir}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    model_checkpoint = BertForSequenceClassification.from_pretrained(checkpoint_path)\n",
        "\n",
        "    trainer_checkpoint = Trainer(\n",
        "        model=model_checkpoint,\n",
        "        args=training_args,\n",
        "        tokenizer=tokenizer,\n",
        "        eval_dataset=val_dataset\n",
        "    )\n",
        "\n",
        "    metrics = evaluate_model(trainer_checkpoint, val_dataset, f\"Epoch {i}\")\n",
        "    metrics['epoch'] = i\n",
        "    metrics['checkpoint'] = checkpoint_dir\n",
        "    epoch_results.append(metrics)\n",
        "\n",
        "# Summary comparison across all epochs\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SUMMARY: Performance Across All Epochs\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Epoch':<10} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}\")\n",
        "print(\"-\"*70)\n",
        "for result in epoch_results:\n",
        "    print(f\"{result['epoch']:<10} {result['accuracy']:<12.4f} {result['precision']:<12.4f} \"\n",
        "          f\"{result['recall']:<12.4f} {result['f1']:<12.4f}\")\n",
        "\n",
        "# Find best epoch\n",
        "best_epoch = max(epoch_results, key=lambda x: x['f1'])\n",
        "print(f\"\\n Best performing epoch: Epoch {best_epoch['epoch']} (F1-Score: {best_epoch['f1']:.4f})\")\n"
      ],
      "metadata": {
        "id": "oBec5etc4JTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GCZXaFD9JTUT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}