{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anmollllll/SuicideTextDetection_and_ResponseModel/blob/main/ResponseBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xj9c_Mo2--3z"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit\n",
        "!pip install  langchain-google-genai\n",
        "!pip install langchain-core\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyG0gNHhZeXZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSjN8pB-Vad2",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "# page setup\n",
        "st.set_page_config(page_title=\"Crisis Support Chatbot\", page_icon=\"üí¨\", layout=\"wide\")\n",
        "st.title(\"üß† Suicide Risk Detection & Support Chatbot\")\n",
        "\n",
        "\n",
        "BERT_MODEL_PATH = \"/content/drive/MyDrive/SusDetect/results/checkpoint-4269\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"...........your_api_code.........\"\n",
        "Chatmodel = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "\n",
        "# system_prompt\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a compassionate AI crisis support companion designed to provide immediate emotional support to individuals experiencing suicidal thoughts or mental health crises.\n",
        "\n",
        "Your Core Responsibilities:\n",
        "\n",
        "1. Soothe and Stabilize\n",
        "- Use gentle, calming language to help reduce immediate distress\n",
        "- Suggest grounding techniques like deep breathing or mindfulness when appropriate\n",
        "- Help the person feel heard and less alone in their struggle\n",
        "\n",
        "2. Build Trust Through Empathy\n",
        "- Be non-judgmental, warm, and accepting of all feelings\n",
        "- Validate their pain without minimizing it\n",
        "- Listen actively and reflect back what you hear to show understanding\n",
        "- Maintain confidentiality in our conversation\n",
        "\n",
        "3. Understand the Crisis\n",
        "- Ask open-ended questions to gently explore what's happening\n",
        "- Identify specific triggers, stressors, or situations contributing to their pain\n",
        "- Acknowledge the complexity of their experience\n",
        "\n",
        "4. Collaborate on Coping Strategies\n",
        "- Help them identify their existing strengths and past coping mechanisms\n",
        "- Explore small, manageable steps they can take right now\n",
        "- Encourage problem-solving without forcing solutions\n",
        "- Discuss people or resources they might reach out to\n",
        "\n",
        "5. Encourage Safety and Professional Support\n",
        "- Gently encourage them to commit to staying safe\n",
        "- Remind them that professional help is available and valuable\n",
        "- Provide crisis helpline information when appropriate\n",
        "\n",
        "\n",
        "Crisis Resources to Share When Needed:\n",
        "Direct the users to one of these website links or based on which country they are currently on,find the emergency numbers from these websites and help them.\n",
        "https://heartfeltpresence.org/global-suicide-hotlines/?utm_source=chatgpt.com \"Global Emergency Hotlines - Heartfelt Presence\"\n",
        "https://trystressmanagement.com/mental-health-resources/asias-hotlines/?utm_source=chatgpt.com \"Asia's Crisis Centres and Hotlines - Try Stress Management\"\n",
        "https://www.positivminds.com/te/general-5?utm_source=chatgpt.com \"Help | PositivMinds\"\n",
        "\n",
        "\n",
        "Tone and Approach:\n",
        "- Be warm, genuine, and human in your responses\n",
        "- Avoid clinical jargon or overly formal language\n",
        "- Show care through your words, not just procedures\n",
        "- Balance hope with realism - don't offer toxic positivity\n",
        "- Meet them where they are emotionally\n",
        "\n",
        "MOST IMPORTANT NOTE:\n",
        "Always provide positive response to the user input and be positive to the user no matter how the critical situtation is.\n",
        "\"\"\"\n",
        "\n",
        "#session state init\n",
        "if \"post_input\" not in st.session_state:\n",
        "    st.session_state.post_input = \"\"\n",
        "if \"chat_active\" not in st.session_state:\n",
        "    st.session_state.chat_active = False\n",
        "if \"current_chat_messages\" not in st.session_state:\n",
        "    st.session_state.current_chat_messages = []\n",
        "if \"active_post_text\" not in st.session_state: # Stores the original post that started the current chat\n",
        "    st.session_state.active_post_text = \"\"\n",
        "if \"chat_history\" not in st.session_state:\n",
        "    st.session_state.chat_history = []  # list of dicts: {id, post, chat_messages, ts}\n",
        "if \"selected_history_id\" not in st.session_state:\n",
        "    st.session_state.selected_history_id = None\n",
        "if \"chat_input_key\" not in st.session_state: # Used to reset the chat text_input\n",
        "    st.session_state.chat_input_key = 0\n",
        "if \"clear_post_input_flag\" not in st.session_state: # NEW: Flag to control post_input clearing\n",
        "    st.session_state.clear_post_input_flag = False\n",
        "if \"last_non_suicidal_prediction\" not in st.session_state:\n",
        "    st.session_state.last_non_suicidal_prediction = None # to store {label, prob}\n",
        "\n",
        "# This ensures st.session_state.post_input is only modified BEFORE the widget is created.\n",
        "if st.session_state.clear_post_input_flag:\n",
        "    st.session_state.post_input = \"\"\n",
        "    st.session_state.clear_post_input_flag = False\n",
        "\n",
        "# loading bert model\n",
        "@st.cache_resource\n",
        "def load_bert_model(path):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(path)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(path)\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, bert_model = load_bert_model(BERT_MODEL_PATH)\n",
        "\n",
        "def predict_suicidal(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    outputs = bert_model(**inputs)\n",
        "    probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "    label_idx = torch.argmax(probs).item()\n",
        "    suicidal_prob = float(probs[0][1].item())\n",
        "    label = \"Suicidal\" if label_idx == 1 else \"Non-Suicidal\"\n",
        "    return label, suicidal_prob\n",
        "\n",
        "#chat interaction functions\n",
        "\n",
        "def start_new_chat_session(post_text):\n",
        "    \"\"\"Initializes a new chat session with the bot based on the user's post.\"\"\"\n",
        "    st.session_state.current_chat_messages = [\n",
        "        SystemMessage(content=SYSTEM_PROMPT),\n",
        "        HumanMessage(content=post_text)\n",
        "    ]\n",
        "    with st.spinner(\"Connecting to bot...\"):\n",
        "        bot_response = Chatmodel.invoke(st.session_state.current_chat_messages)\n",
        "    st.session_state.current_chat_messages.append(AIMessage(content=bot_response.content))\n",
        "    st.session_state.chat_active = True\n",
        "    st.session_state.active_post_text = post_text\n",
        "    st.session_state.chat_input_key += 1\n",
        "    st.rerun()\n",
        "\n",
        "def send_chat_message(user_message):\n",
        "    \"\"\"Sends a user message and gets a bot response in an active chat.\"\"\"\n",
        "    if not user_message.strip():\n",
        "        return\n",
        "\n",
        "    st.session_state.current_chat_messages.append(HumanMessage(content=user_message))\n",
        "    with st.spinner(\"Bot is typing...\"):\n",
        "        bot_response = Chatmodel.invoke(st.session_state.current_chat_messages)\n",
        "    st.session_state.current_chat_messages.append(AIMessage(content=bot_response.content))\n",
        "    st.session_state.chat_input_key += 1\n",
        "    st.rerun()\n",
        "\n",
        "def end_chat_session_logic(trigger_clear_post_input=False):\n",
        "    \"\"\"\n",
        "    Handles the logic of ending a chat and saving to history.\n",
        "    Sets a flag if post_input should be cleared on the next rerun.\n",
        "    \"\"\"\n",
        "    if st.session_state.chat_active and st.session_state.current_chat_messages:\n",
        "        chat_for_history = [msg for msg in st.session_state.current_chat_messages if not isinstance(msg, SystemMessage)]\n",
        "\n",
        "        entry = {\n",
        "            \"id\": len(st.session_state.chat_history) + 1,\n",
        "            \"post\": st.session_state.active_post_text,\n",
        "            \"chat_messages\": chat_for_history,\n",
        "            \"ts\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        }\n",
        "        st.session_state.chat_history.insert(0, entry)\n",
        "\n",
        "    # Reset active chat state\n",
        "    st.session_state.chat_active = False\n",
        "    st.session_state.current_chat_messages = []\n",
        "    st.session_state.active_post_text = \"\"\n",
        "    st.session_state.selected_history_id = None\n",
        "    st.session_state.chat_input_key += 1\n",
        "\n",
        "    if trigger_clear_post_input:\n",
        "        st.session_state.clear_post_input_flag = True\n",
        "\n",
        "def handle_exit_chat_button_click():\n",
        "    \"\"\"Callback for the Exit Chat button.\"\"\"\n",
        "    end_chat_session_logic(trigger_clear_post_input=True)\n",
        "    st.success(\"Chat ended and saved to history.\")\n",
        "    st.rerun()\n",
        "\n",
        "def load_chat_from_history(history_item):\n",
        "    \"\"\"Loads a selected chat from history to view or continue.\"\"\"\n",
        "    st.session_state.current_chat_messages = [SystemMessage(content=SYSTEM_PROMPT)] + history_item[\"chat_messages\"]\n",
        "    st.session_state.active_post_text = history_item[\"post\"]\n",
        "    st.session_state.chat_active = True\n",
        "    st.session_state.selected_history_id = history_item[\"id\"]\n",
        "    st.session_state.post_input = history_item[\"post\"] # Pre-fill post input if continuing\n",
        "    st.session_state.chat_input_key += 1\n",
        "    st.rerun()\n",
        "\n",
        "def delete_chat_from_history(history_id):\n",
        "    \"\"\"Deletes a chat entry from the history.\"\"\"\n",
        "    was_active_chat_deleted = (st.session_state.selected_history_id == history_id) or \\\n",
        "                             (st.session_state.chat_active and any(item[\"id\"] == history_id for item in st.session_state.chat_history if item[\"post\"] == st.session_state.active_post_text))\n",
        "\n",
        "    st.session_state.chat_history = [\n",
        "        item for item in st.session_state.chat_history if item[\"id\"] != history_id\n",
        "    ]\n",
        "    if was_active_chat_deleted:\n",
        "        end_chat_session_logic(trigger_clear_post_input=True) # End current chat and clear post_input\n",
        "    else:\n",
        "        pass\n",
        "    st.rerun()\n",
        "\n",
        "# UI Layout\n",
        "\n",
        "# Sidebar for Chat History\n",
        "with st.sidebar:\n",
        "    st.header(\"üóÇÔ∏è Chat History\")\n",
        "    if st.session_state.chat_history:\n",
        "        for i, chat_entry in enumerate(st.session_state.chat_history):\n",
        "            is_current_active_chat = (st.session_state.chat_active and st.session_state.active_post_text == chat_entry[\"post\"] and\n",
        "                                      [msg for msg in st.session_state.current_chat_messages if not isinstance(msg, SystemMessage)] == chat_entry[\"chat_messages\"])\n",
        "            is_selected_in_history = (st.session_state.selected_history_id == chat_entry[\"id\"])\n",
        "\n",
        "            with st.expander(\n",
        "                f\"**{chat_entry['post'][:45]}...** (ID: {chat_entry['id']}) - {chat_entry['ts']}\",\n",
        "                expanded=is_current_active_chat or is_selected_in_history\n",
        "            ):\n",
        "                st.markdown(f\"**Original Post:** {chat_entry['post']}\")\n",
        "                for msg in chat_entry[\"chat_messages\"]:\n",
        "                    if isinstance(msg, HumanMessage):\n",
        "                        st.markdown(f\"**You:** {msg.content}\")\n",
        "                    elif isinstance(msg, AIMessage):\n",
        "                        st.markdown(f\"**Bot:** {msg.content}\")\n",
        "\n",
        "                col1, col2 = st.columns([1,1])\n",
        "                with col1:\n",
        "                    if st.button(\"Continue Chat\", key=f\"continue_{chat_entry['id']}\"):\n",
        "                        load_chat_from_history(chat_entry)\n",
        "                with col2:\n",
        "                    if st.button(\"Delete\", key=f\"delete_history_{chat_entry['id']}\"):\n",
        "                        delete_chat_from_history(chat_entry[\"id\"])\n",
        "    else:\n",
        "        st.info(\"No saved chats yet.\")\n",
        "\n",
        "# Main content area\n",
        "st.subheader(\"üí¨ Enter a post or text to analyze\")\n",
        "\n",
        "st.text_area(\n",
        "    \"Post / status / text\",\n",
        "    value=st.session_state.post_input,\n",
        "    height=150,\n",
        "    key=\"post_input\"\n",
        ")\n",
        "\n",
        "if st.button(\"üîç Analyze Post\", key=\"analyze_button\"):\n",
        "    if not st.session_state.post_input.strip():\n",
        "        st.warning(\"Please enter some text to analyze.\")\n",
        "    else:\n",
        "        label, suicidal_prob = predict_suicidal(st.session_state.post_input.strip())\n",
        "\n",
        "        st.markdown(f\"### üß† Prediction: **{label}**\")\n",
        "        st.markdown(f\"**Suicidal probability:** {suicidal_prob*100:.2f}%\")\n",
        "        st.progress(suicidal_prob)\n",
        "\n",
        "        if label == \"Suicidal\":\n",
        "            st.error(\"‚ö†Ô∏è Suicidal content detected ‚Äî initiating support chat.\")\n",
        "            start_new_chat_session(st.session_state.post_input.strip())\n",
        "        else:\n",
        "            st.success(\"‚úÖ Non-suicidal detected ‚Äî no chat started.\")\n",
        "            st.session_state.last_non_suicidal_prediction = {\n",
        "                \"label\":label,\n",
        "                \"prob\":suicidal_prob\n",
        "            }\n",
        "\n",
        "# Active Chatbot Interface (appears only when chat_active is True)\n",
        "if st.session_state.chat_active:\n",
        "    st.markdown(\"---\")\n",
        "    st.markdown(\"## ü§ñ Crisis Support Chatbot\")\n",
        "    st.info(f\"Chat initiated for post: **{st.session_state.active_post_text[:100]}...**\")\n",
        "\n",
        "    chat_display_messages = [\n",
        "        msg for msg in st.session_state.current_chat_messages if not isinstance(msg, SystemMessage)\n",
        "    ]\n",
        "    for msg in chat_display_messages:\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            with st.chat_message(\"user\"):\n",
        "                st.markdown(msg.content)\n",
        "        elif isinstance(msg, AIMessage):\n",
        "            with st.chat_message(\"assistant\"):\n",
        "                st.markdown(msg.content)\n",
        "\n",
        "    # Move st.chat_input OUTSIDE the st.form\n",
        "    user_message = st.chat_input(\"Your message:\", key=f\"chat_input_{st.session_state.chat_input_key}\")\n",
        "\n",
        "    if user_message:\n",
        "        send_chat_message(user_message)\n",
        "\n",
        "    with st.form(\"exit_chat_form\", clear_on_submit=False):\n",
        "        if st.form_submit_button(\"‚ùå Exit Chat\"):\n",
        "            handle_exit_chat_button_click()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5-B08MUdNhu"
      },
      "outputs": [],
      "source": [
        "!ngrok authtoken #your ngrok api key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pwo_iViJWAlq"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "\n",
        "ngrok.kill()\n",
        "\n",
        "# Start new tunnel on port 8501\n",
        "public_url = ngrok.connect(addr=\"8501\", proto=\"http\")\n",
        "print(\"Streamlit App URL:\", public_url)\n",
        "\n",
        "!streamlit run app.py --server.port 8501 --server.address 0.0.0.0"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNH8jexuHWCbhrsTNhxjHsA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}